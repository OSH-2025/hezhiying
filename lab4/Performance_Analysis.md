# 一、性能指标列表及合理性说明

| 编号 | 指标名称             | 英文名                     | 含义描述                                                         |
|------|----------------------|----------------------------|------------------------------------------------------------------|
| 1    | 输出速度（TPS）      | Tokens Per Second          | 单位时间内生成的 Token 数量，反映整体推理速度                    |
| 2    | 首 Token 返回延迟    | First Token Latency        | 用户输入后第一个 Token 返回的时间，影响交互体验                  |
| 3    | 上下文加载时间       | Context Load Time          | 加载历史上下文所需时间，影响连续对话流畅性                       |
| 4    | 内存占用峰值         | Peak Memory Usage          | 推理过程中内存使用的最大值，决定资源消耗上限                     |
| 5    | 批处理效率           | Batch Processing Efficiency| 多请求并发时的吞吐效率，体现系统负载能力                         |
| 6    | 正确率               | Accuracy                   | 对于用户输入能获得合理答案的能力                                 |

### 合理性说明：
- **输出速度（TPS）** 是衡量模型推理效率的核心指标。
- **首 Token 返回延迟** 直接影响用户对响应快慢的感知。
- **上下文加载时间** 在对话型应用中尤为重要，长对话需频繁加载上下文。
- **内存占用峰值** 反映部署成本和稳定性，尤其在单机部署时是关键限制。
- **批处理效率** 在多用户场景中决定了服务能否承载高并发。
- **正确率** 是使用模型的核心之一，直接影响用户体验与任务完成质量。

---

# 测试任务以及实际部署模型测试

## 测试任务包括：
- 输出速度（TPS）
- 内存（显存）占用峰值

## 实际部署模型测试示例

以 **Qwen2.5 多模态模型** 在 OKVQA 数据集 `val` 部分的推断为例，其中本次代码是在具有50G显存的L20的GPU上进行的测试：

### 测试环境与配置：
- 数据总量：5046 个“问题-图片”对
- 图片大小范围：500KB - 3000KB
- 问题长度：5 - 40 个单词
- 使用工具：HuggingFace Transformers 原生 API（未使用 vLLM 加速）
- 总耗时：3 小时 42 分钟 ≈ **13,320 秒**
- 生成文件大小：约 1.92MB

### 关键性能指标：
| 指标 | 数值 | 说明 |
|------|------|------|
| 平均处理时间 per 样本 | 2.64 秒/样本 | 包括图像处理 + 推理 + 生成 |
| 总 token 数估算 | ~503,316 tokens | 根据输出文件大小估算 |
| 平均时间 per token | 26.5 ms/token | 每个 token 生成所需时间 |
| 推理吞吐量 TPS | 37.7 tokens/s | 每秒可生成约 37.7 个 token |
| 显存占用峰值 | 约 25GB | 主要用于模型加载和缓存 |

### 使用 vLLM 加速后的效果：
- 显存占用峰值飙升至约 **44GB**
- 总耗时缩短至 **2 小时 37 分钟**
- 主要优化原因：vLLM 的缓存机制提升了 KV Cache 的复用效率，显著减少了图像与问题处理时间

---
# 参数调试与分析

在使用 Qwen 系列模型对 OKVQA 数据集验证集部分进行推理任务（如 **束搜索（beam search）** 和 **多采样生成多样化关系路径及思维链（Chain-of-Thought）**）时，多个关键参数会对性能指标产生显著影响。以下是对主要参数的调试说明及其对性能的影响分析。

---

# 一、关键参数及其性能影响分析

### 1. `beam_num`（束搜索宽度）

- **作用**：控制束搜索过程中保留的最佳候选路径数量。
- **影响**：
    - **输出速度（TPS）**      
      更大的 `beam_num` 意味着需要处理更多候选序列，计算量成倍增长，显著降低 TPS。
    - **首 Token 返回延迟**     
      初始 token 的生成虽然不受直接影响，但整体解码路径变复杂，略微增加首次响应时间。
    - **内存占用峰值**      
      每个 beam 都需维护独立的 KV Cache，显存占用大幅提升。
    - **正确率（Accuracy）**      
      更宽的搜索路径有助于提升答案质量，尤其在逻辑推理类任务中效果明显。

 对于追求效率的场景，可适当减小 `beam_num`；对于OKVQA这样的数据集我在尝试进行多样化生成时，为了追求多样性适度调大了该值。


### 2. `max_new_tokens`（最大生成长度）

- **作用**：限制模型一次推理中最多生成的新 token 数量。
- **影响**：
    - **输出速度（TPS）**     
      单次任务的总耗时随 token 数增加而线性增长，但每 token 的生成速度不变。
    - **首 Token 返回延迟**    
      不直接影响首 token 延迟。
    - **任务完成时间**      
      生成越长的内容，整个请求耗时越久。
    - **资源占用**      
      更长的输出意味着更大的显存开销，尤其是在 batch 场景下。

根据实际任务需求设定合理的生成长度.在我的测试中，模型的参数量与这个值也是相关性比较强的，google的gemma模型（12B）比qwen的7B生成的文本内容显著要多很多。而且很多时候感觉模型对于某个问题有着属于自己的完整的话，设置少只是把这句话给截断了而不是限制模型在这么多tokens的情况下回答我的问题。



### 3. `temperature`（温度）

- **作用**：控制输出随机性的强度，用于调节生成内容的多样性和确定性。
- **影响**：
    - **输出速度（TPS）**     
      温度越高，采样过程越复杂（如 multinomial sampling），可能降低吞吐量。
    - **首 Token 返回延迟**     
      第一个 token 的生成涉及采样逻辑，高温度可能略微延缓返回时间。
    - **批处理效率**      
      多个请求的采样行为不同步，导致 batch 利用率下降。
    - **输出质量**      
      较低温度适合精准任务，较高温度适合创意任务。

对于OKVQA数据集上的测试来言，跟TOPK或者TOPP差不多，因为都是围绕问题来展开，而非用户输入一个开放性的问题来不断解答。
      

### 4. `top_k`（Top-k 采样）

- **作用**：限制采样仅从概率最高的前 k 个 token 中选择。
- **影响**：
    - **输出速度（TPS）**      
      当 `k` 较大时，排序和采样操作显著增加计算开销，降低 TPS。
    - **输出质量**      
      控制生成多样性，过高可能导致无意义输出，过低则限制创造力。
    - **显存占用**     
      对显存影响较小，但会增加中间计算步骤。



### 5. `top_p`（Nucleus 采样 / Top-p 采样）

- **作用**：基于累积概率选择最小词集进行采样，动态调整词汇范围。
- **影响**：
    - **输出速度（TPS）**     
      相比 `top_k` 更灵活，但仍会带来额外计算开销。
    - **输出质量**      
      能在保持一定多样性的同时保证语言流畅性。
    - **显存占用**    
      影响较小。

适用于生成任务，推荐结合 `top_k` 使用。

个人感觉这个参数和上述参数一样，在OKVQA数据集上进行测试的时候，两者进行相当程度的调节，发现正确率没有多大的改变，可能是因为不是创作型的任务而是对于固定的问题进行回答，因此只需要模型对固定问题能给出答案空间附近的向量。

---

# 优化操作

为了在不影响输出质量的前提下提升推理效率，我推荐以下两项关键优化操作：


## 优化操作一：启用 Prefix Caching（前缀缓存机制）

Prefix Caching 是现代推理框架（如 vLLM）提供的高级功能，其核心思想是：

**对多个请求中相同的输入前缀（prompt 或 history）进行共享缓存，避免重复计算对应的 Key-Value Cache（KV Cache）**。

这在如下场景中尤为有效：
- 多用户共用相同系统提示；
- 同一批任务包含重复的历史上下文；
- Agent 系统中多个子任务共享固定 prompt。

###   对性能指标的影响

| 指标名称            | 原因说明 |
|-----------------------|----------|
| 输出速度（TPS）             | 显著提升吞吐量，减少重复计算，提高 GPU 利用率。测试显示可提升约 20%~50% 的 TPS。 |
| 首 Token 返回延迟          | 减少了重复的 KV Cache 构建时间，首次响应更快。 |
| 批处理效率                 | 提升 batch 内部利用率，相同时间内可处理更多请求。 |
| 内存占用峰值                | 缓存复用减少了冗余 KV Cache 存储，显存占用略有下降。 |
| 正确率（Accuracy）           | 不改变生成逻辑，不影响输出内容，因此无直接影响。 |

### 对输出质量的影响

- **不牺牲输出质量**：Prefix Caching 仅优化了相同前缀部分的计算过程，并不改变模型的解码策略和生成路径。
- **不影响采样参数**：只要 temperature、top-k/p 等参数一致，不同请求仍可保持多样性。
- **支持复杂对话结构**：适用于具有共享系统提示、角色设定等固定前缀的对话系统。

## 优化操作二:启用连续批处理（Continuous Batching）

在处理大量并发请求时，连续批处理（Continuous Batching） 可大幅提高推理吞吐能力。

它通过 动态将新到达的请求加入当前正在运行的 batch 中，避免 batch 空转，从而充分利用 GPU 资源。有点类似cpu中的流水线、

###  对性能指标的影响

指标名称|	原因说明
|---|--|
输出速度（TPS）|		提升 batch 利用率，显著提高单位时间内处理的 token 数量。
批处理效率|	动态调度请求，提升并发处理能力。
首 Token 返回延迟|		由于 batch 平摊计算成本，单个请求延迟略有上升。
内存占用峰值|		不直接影响显存占用。

## 优化操作三：KV Cache（键值缓存）
原理：在 Transformer 模型中，自注意力机制需要计算 Key 和 Value 向量（即 KV 对），并在后续层中使用这些向量进行计算。对于相同的输入序列，KV 对是固定的，因此可以通过缓存这些值来避免重复计算。

显存占用：KV Cache 需要额外的显存来存储中间结果。随着批处理大小或序列长度的增加，所需显存也会增加。

性能提升：通过缓存 KV 对，减少了重复计算的时间开销，尤其是在处理长序列或多轮对话时，这种优化可以显著提高推理速度


